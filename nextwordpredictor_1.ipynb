{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nextwordpredictor_1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMVBA1nnyhKZV0bIFQxJNkm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Onkar-stac/Next-Word-Predictor/blob/main/nextwordpredictor_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lx025vcrzkSs"
      },
      "outputs": [],
      "source": [
        "#import library\n",
        "import re\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense,Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data to extract\n",
        "url = \"https://www.gutenberg.org/cache/epub/5200/pg5200.txt\""
      ],
      "metadata": {
        "id": "nIj3tVMhztR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to extract data from the url\n",
        "def get_book(url):\n",
        "  raw = requests.get(url).text\n",
        "  #discarding the beginning of the data\n",
        "  start = re.search(r\"\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK .* \\*\\*\\*\",raw).end()\n",
        "  #discarding the end of the data\n",
        "  stop=re.search(r\"II\", raw).start()\n",
        "  text=raw[start:stop] #relevant data\n",
        "  return text"
      ],
      "metadata": {
        "id": "h11s0Uff0QPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#processing\n",
        "def preprocess(sentence):\n",
        "  return re.sub('[^A-Za-z0-9.]+',' ',sentence).lower()"
      ],
      "metadata": {
        "id": "CBhtb-YR0QUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calling the functions\n",
        "book = get_book(url)\n",
        "processed_book=preprocess(book)\n",
        "print(processed_book)"
      ],
      "metadata": {
        "id": "sisfba3D0RAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(processed_book)"
      ],
      "metadata": {
        "id": "H-UWsSWY0phQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EDA\n",
        "len(re.findall(r'the',processed_book))\n",
        "processed_book = re.sub(r'\\si\\s', \" I \",processed_book)\n",
        "processed_book = re.sub(r'[^\\w\\s]',\" \",processed_book)\n",
        "print(processed_book)"
      ],
      "metadata": {
        "id": "OEiOcGy90uT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([processed_book])\n",
        "#saving the tokenizer\n",
        "pickle.dump(tokenizer,open('token.pkl','wb'))\n",
        "sequence_data = tokenizer.texts_to_sequences([processed_book])[0] #vectorize the text corpus\n",
        "sequence_data[:15]"
      ],
      "metadata": {
        "id": "oNx9T_cA0zU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sequence_data)"
      ],
      "metadata": {
        "id": "6hHgdN0802id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index)+1 #number of unique words in the text corpus\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "pZVu4GYr04sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = []\n",
        "for i in range(3,len(sequence_data)):\n",
        "  words = sequence_data[i-3:i+1]\n",
        "  sequences.append(words)\n",
        "print(\"The length of sequences are: \",len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "sequences[:10]"
      ],
      "metadata": {
        "id": "L5VQO34607eP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=[]\n",
        "Y=[]\n",
        "for i in sequences:\n",
        "  X.append(i[0:3])\n",
        "  Y.append(i[3])\n",
        "X=np.array(X)\n",
        "Y=np.array(Y)\n"
      ],
      "metadata": {
        "id": "tkmU86QA1oxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y=to_categorical(Y,num_classes=vocab_size)\n",
        "Y[:5]"
      ],
      "metadata": {
        "id": "EujsfvDV1vvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size,10,input_length=3))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(vocab_size,activation=\"softmax\"))"
      ],
      "metadata": {
        "id": "zTMFEKFl12uT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "hYX5EmA64RBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "file_name_path=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(file_name_path, monitor='loss', \n",
        "verbose=1, save_best_only=True, mode='min')\n",
        "model.compile(loss=\"categorical_crossentropy\",optimizer=Adam(learning_rate=0.001))\n",
        "callbacks = [checkpoint]"
      ],
      "metadata": {
        "id": "yq4GzwGZ4UuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, Y, epochs=50, batch_size=64, callbacks=callbacks) "
      ],
      "metadata": {
        "id": "34qH-wGi4ezn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "model = load_model('weights-improvement-50-0.6095.hdf5')\n",
        "tokenizer = pickle.load(open('token.pkl','rb'))\n",
        "\n",
        "def Predict_Next_Words(model,tokenizer,text):\n",
        "  sequence = tokenizer.texts_to_sequences([text])\n",
        "  sequence = np.array(sequence)\n",
        "  preds = np.argmax(model.predict(sequence))\n",
        "  predicted_word = \"\"\n",
        "\n",
        "  for key,value in tokenizer.word_index.items():\n",
        "    if value==preds:\n",
        "      predicted_word=key\n",
        "      break\n",
        "  print(predicted_word)\n",
        "  return predicted_word"
      ],
      "metadata": {
        "id": "LMu1wBiS5KeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while(True):\n",
        "  text = input(\"Enter: \")\n",
        "  if text==\"0\":\n",
        "    print(\"Execution completed\")\n",
        "    break\n",
        "  else:\n",
        "    text=text.split(\" \")\n",
        "    text=text[-3:]\n",
        "    print(text)\n",
        "    Predict_Next_Words(model,tokenizer,text)"
      ],
      "metadata": {
        "id": "vdFJ9dGm5XQ2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}